# Task 9 â€” Persistence Boundary & Startup Recovery

## Overview

This task introduces **startup recovery and persistence boundary** to the job queue system. The focus is on defining what happens when the process restarts: ensuring no jobs are lost, no jobs are stuck in inconsistent states, and the system can resume work from where it left off.

## âœ… Completed Requirements

### Functional Requirements

- âœ… Recovery logic implemented
- âœ… Processing jobs moved back to pending on startup
- âœ… Pending jobs re-enqueued on startup
- âœ… Recovery respects backpressure (no jobs dropped)
- âœ… Recovery completes before workers start
- âœ… Completed and failed jobs remain untouched

### Technical Requirements

- âœ… Store is source of truth
- âœ… Queue is delivery mechanism
- âœ… Recovery logic in separate package
- âœ… Recovery uses same state transition rules
- âœ… Correct startup order
- âœ… GetProcessingJobs method added to store
- âœ… processing â†’ pending transition allowed

## ğŸ“ Project Structure

```
job-queue-backend/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go              # Recovery call, startup order
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ recovery/
â”‚   â”‚   â””â”€â”€ recovery.go          # Recovery logic (NEW)
â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â””â”€â”€ job_store.go         # GetProcessingJobs, canTransition update
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ task9/
â”‚   â”‚   â”œâ”€â”€ README.md            # This file
â”‚   â”‚   â”œâ”€â”€ summary.md           # Quick reference
â”‚   â”‚   â”œâ”€â”€ description.md       # Task requirements
â”‚   â”‚   â””â”€â”€ concepts/            # Detailed concept explanations
â”‚   â””â”€â”€ learnings.md            # Overall learnings
â””â”€â”€ go.mod                      # Go module
```

**Structure improvements:**
- Recovery package separated from HTTP and workers
- Recovery logic in dedicated package
- Clear separation of concerns

## ğŸ”‘ Key Concepts Learned

### 1. Startup Recovery

- **What**: Restoring system state when application restarts
- **Why**: Ensures no jobs lost, no jobs stuck, system can resume
- **How**: Read from store, update state, re-enqueue jobs
- **Pattern**: Store â†’ Recovery â†’ Queue â†’ Workers

### 2. Source of Truth Design

- **What**: Store is authoritative, queue is delivery mechanism
- **Why**: Clear ownership, enables persistence, simplifies recovery
- **How**: Recovery always starts from store, never from queue
- **Pattern**: Store owns data, queue owns delivery

### 3. Recovery Backpressure

- **What**: Handling queue full during recovery
- **Why**: Cannot drop jobs, must respect queue capacity
- **How**: Exponential backoff with retries
- **Pattern**: Try â†’ Wait â†’ Retry â†’ Success or fail after max attempts

### 4. State Transitions in Recovery

- **What**: Recovery must use same state transition rules
- **Why**: Consistency, validation, encapsulation
- **How**: Use UpdateStatus, add recovery transitions
- **Pattern**: All state changes go through validation

## ğŸ“ Implementation Details

### Recovery Function

```go
func RecoverJobs(
    ctx context.Context,
    jobStore store.JobStore,
    jobQueue chan string,
    logger *slog.Logger,
) error {
    logger.Info("Starting recovery", "event", "recovery_started")

    // Step 1: Move processing jobs back to pending
    processingJobs, err := jobStore.GetProcessingJobs(ctx)
    if err != nil {
        return fmt.Errorf("failed to get processing jobs: %w", err)
    }

    processingRecovered := 0
    for _, job := range processingJobs {
        err := jobStore.UpdateStatus(ctx, job.ID, domain.StatusPending, nil)
        if err != nil {
            logger.Error("Failed to recover processing job", ...)
            continue
        }
        processingRecovered++
        logger.Info("Recovered processing job", ...)
    }

    // Step 2: Re-enqueue all pending jobs
    pendingJobs, err := jobStore.GetPendingJobs(ctx)
    if err != nil {
        return fmt.Errorf("failed to get pending jobs: %w", err)
    }

    pendingReEnqueued := 0
    for _, job := range pendingJobs {
        if err := reEnqueueWithBackpressure(ctx, job.ID, jobQueue, logger); err != nil {
            return fmt.Errorf("failed to re-enqueue job %s: %w", job.ID, err)
        }
        pendingReEnqueued++
    }

    logger.Info("Recovery completed", ...)
    return nil
}
```

**Key points:**
- Two-step process: move processing â†’ pending, then re-enqueue
- Uses UpdateStatus to respect state transitions
- Handles errors gracefully (continues on individual failures)
- Logs recovery progress

### Startup Order

```go
func main() {
    // 1. Initialize store
    jobStore := store.NewInMemoryJobStore()
    metricStore := store.NewInMemoryMetricStore()

    // 2. Initialize queue (needed for recovery)
    jobQueue := make(chan string, config.JobQueueCapacity)

    // 3. Run recovery logic (BEFORE workers start)
    recoveryCtx := context.Background()
    if err := recovery.RecoverJobs(recoveryCtx, jobStore, jobQueue, logger); err != nil {
        log.Fatalf("Recovery failed: %v", err)
    }

    // 4. Start workers (after recovery)
    for i := 0; i < config.WorkerCount; i++ {
        worker := worker.NewWorker(...)
        wg.Go(func() {
            worker.Start(workerCtx)
        })
    }

    // 5. Start HTTP server
    go func() {
        srv.ListenAndServe()
    }()
}
```

**Key points:**
- Recovery runs synchronously (blocks until complete)
- Workers start after recovery completes
- Server starts after workers start
- Correct order ensures no race conditions

### Backpressure During Recovery

```go
func reEnqueueWithBackpressure(
    ctx context.Context,
    jobID string,
    jobQueue chan string,
    logger *slog.Logger,
) error {
    backoff := 50 * time.Millisecond
    maxBackoff := 5 * time.Second
    maxAttempts := 10

    for attempt := 0; attempt < maxAttempts; attempt++ {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case jobQueue <- jobID:
            return nil // Success!
        default:
            if attempt < maxAttempts-1 {
                logger.Info("Queue full during recovery, backing off", ...)
                select {
                case <-ctx.Done():
                    return ctx.Err()
                case <-time.After(backoff):
                    backoff = time.Duration(float64(backoff) * 1.5)
                    if backoff > maxBackoff {
                        backoff = maxBackoff
                    }
                }
            }
        }
    }

    return fmt.Errorf("failed to enqueue job %s after %d attempts", jobID, maxAttempts)
}
```

**Key points:**
- Exponential backoff (starts at 50ms, increases by 1.5x)
- Max backoff capped at 5 seconds
- Max 10 attempts before failing
- Respects context cancellation

### State Transition Update

```go
func canTransition(from, to domain.JobStatus) bool {
    switch {
    case from == domain.StatusPending && to == domain.StatusProcessing:
        return true
    case from == domain.StatusProcessing && to == domain.StatusCompleted:
        return true
    case from == domain.StatusProcessing && to == domain.StatusFailed:
        return true
    case from == domain.StatusFailed && to == domain.StatusPending:
        return true
    case from == domain.StatusProcessing && to == domain.StatusPending:
        return true // â† ADDED for recovery
    default:
        return false
    }
}
```

**Key points:**
- Added `processing â†’ pending` transition
- Only used during recovery
- Allows recovery to move interrupted jobs back to pending

## ğŸš€ Running the Service

### Build

```bash
go build -o bin/server ./cmd/server
```

### Run

```bash
# Default settings
go run ./cmd/server

# Custom configuration
PORT=3000 WORKER_COUNT=20 JOB_QUEUE_CAPACITY=200 go run ./cmd/server
```

### Test Recovery

```bash
# Start server
go run ./cmd/server

# Create some jobs
for i in {1..10}; do
  curl -X POST http://localhost:8080/jobs \
    -H "Content-Type: application/json" \
    -d '{"type": "test", "payload": {}}'
done

# Kill server (Ctrl+C or kill -9)
# Restart server
go run ./cmd/server

# Observe: Recovery runs, jobs are re-enqueued
```

## ğŸ“‹ Quick Reference Checklist

### Recovery Implementation

- âœ… Recovery package created
- âœ… RecoverJobs function implemented
- âœ… Processing jobs moved to pending
- âœ… Pending jobs re-enqueued
- âœ… Backpressure handling
- âœ… Error handling

### Store Updates

- âœ… GetProcessingJobs method added
- âœ… canTransition updated for recovery
- âœ… Store remains source of truth

### Startup Order

- âœ… Store initialized first
- âœ… Recovery runs before workers
- âœ… Workers start after recovery
- âœ… Server starts last

### State Transitions

- âœ… processing â†’ pending allowed
- âœ… Recovery uses UpdateStatus
- âœ… No bypassing validation

## ğŸ”„ Recovery Flow

### Normal Startup Flow

```
1. Initialize store
   â””â”€> Store ready

2. Initialize queue
   â””â”€> Queue ready (empty)

3. Run recovery
   â””â”€> Get processing jobs from store
   â””â”€> Move processing â†’ pending
   â””â”€> Get pending jobs from store
   â””â”€> Re-enqueue all pending jobs
   â””â”€> Queue populated

4. Start workers
   â””â”€> Workers process from queue

5. Start HTTP server
   â””â”€> Server accepts requests
```

### Recovery During Queue Full

```
1. Recovery tries to enqueue job
   â””â”€> Queue full

2. Exponential backoff
   â””â”€> Wait 50ms
   â””â”€> Try again

3. Still full
   â””â”€> Wait 75ms
   â””â”€> Try again

4. Success or max attempts
   â””â”€> Job enqueued or error
```

## ğŸ¯ Design Decisions

### Why Recovery Package?

- **Separation of concerns**: Recovery logic separate from HTTP and workers
- **Testability**: Easier to test recovery independently
- **Maintainability**: Clear ownership and responsibility
- **Future-proofing**: Prepares for persistence integration

### Why Store is Source of Truth?

- **Authority**: Store contains all job state
- **Persistence**: Can be persisted (future)
- **Validation**: Enforces state rules
- **Recovery**: Always starts from store

### Why Exponential Backoff?

- **Handles temporary full**: Queue full is usually temporary
- **Starts fast**: Quick initial retry
- **Increases gradually**: Not too aggressive
- **Capped**: Prevents infinite waiting

### Why processing â†’ pending Transition?

- **Recovery need**: Jobs interrupted during crash need retry
- **Correct state**: Pending is correct for unprocessed jobs
- **Normal flow**: Allows normal retry mechanism
- **Clear semantics**: Recovery is special case

## ğŸ”„ Future Improvements

Potential enhancements for future tasks:

- Persistence implementation (disk storage)
- Recovery from persisted store
- Startup recovery metrics
- Recovery timeout configuration
- Partial recovery (recover some jobs even if others fail)
- Recovery progress reporting
- Distributed recovery coordination

## ğŸ“š Additional Notes

- **Go version**: 1.21+ (for `wg.Go()` support)
- **Dependencies**: Standard library only
- **Project structure**: Follows Go best practices
- **Code style**: Idiomatic Go patterns
- **Concurrency**: Safe for concurrent access
- **Storage**: In-memory (temporary, lost on restart)

## âš ï¸ Critical Bugs Avoided

### 1. Jobs Stuck in Processing State
- **Bug**: No recovery, jobs remain in processing forever
- **Fix**: Recovery moves processing â†’ pending
- **Impact**: Jobs never processed after restart

### 2. Workers Start Before Recovery
- **Bug**: Workers start before recovery completes
- **Fix**: Recovery runs synchronously before workers
- **Impact**: Race conditions, unpredictable behavior

### 3. Dropping Jobs During Recovery
- **Bug**: Jobs dropped if queue full
- **Fix**: Exponential backoff with retries
- **Impact**: Jobs lost during recovery

### 4. Bypassing State Transitions
- **Bug**: Direct mutation bypasses validation
- **Fix**: Use UpdateStatus in recovery
- **Impact**: Invalid states, inconsistent behavior

---

For detailed explanations of any concept, see the [concepts documentation](./concepts/README.md).

